# Distributed-Cache
A lightweight, Go-based distributed in-memory cache with  **consistent-hashing sharding**, **singleflight deduplication**, **size-bounded LRU**, **protobuf communication**, and **read-through replication**
## Architecture

```text
Client â†’ Group.Add("ğŸº", "Hymeis")
        â””â”€> Cache.Add("ğŸº", "Hymeis")
             â”œâ”€ insert into in-memory LRU
             â””â”€ async fan-out to R-1 successors:
                  â””â”€ for each replica in GetReplicas("ğŸº", R)[1:]:
                       HTTP POST /dcache/<group>/ğŸº  (SetRequest)

Client â†’ Group.Get("ğŸº")
        â”œâ”€ LRU hit? â”€â”€â–¶ return "Hymeis"
        â””â”€ cache miss:
            â””â”€ singleflight.Do("ğŸº", fn):
                â””â”€ pickPeer("ğŸº") via consistent-hash
                    â”œâ”€ peer? â”€â”€â–¶ peerLoad (HTTP+Protobuf) â”€â”€â–¶ return "Hymeis"
                    â””â”€ local?  â”€â”€â–¶ localLoad:
                         â”œâ”€ GetterFunc â†’ origin data
                         â”œâ”€ Replication() (see Add flow above)
                         â””â”€ return "Hymeis"

```
---
## wrk2 BenchMark Testing
Sustained a constant 15â€¯000 QPS workload with wrk2, observing:
- Throughput: 14â€¯982 req/sec (â‰ˆ15â€¯000 target)
- Mean latency: 0.868â€¯ms
- P50 / P75 / P90: 0.86â€¯ms / 1.15â€¯ms / 1.44â€¯ms
- P99: 1.94â€¯ms (well under 10â€¯ms SLO)
- P99.9 / P99.99: 2.40â€¯ms / 2.86â€¯ms
- Max observed: 4.00â€¯ms

---
## Next Steps
- Application side: Maybe make a Leetcode Top K ranking system? 
# How to run the project
Try
```
bash run.sh
```
and check the output shown